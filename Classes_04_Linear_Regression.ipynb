{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abelowska/dataPy/blob/main/Classes_04_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression: transforming data"
      ],
      "metadata": {
        "id": "0UM_-fA16Wx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we are going to use our own dataset.\n",
        "\n",
        "The dataset consists of data on **personality** (Big Five assesed with [NEO FFI](https://sjdm.org/dmidi/NEO-FFI.html)) and **cognitive religious belief styles** ([The Post-Critical Belief Scale](https://theo.kuleuven.be/apps/press/ecsi/files/2019/03/4.-Pollefeyt-Bouwens-PCB-Melb-Vict-for-dummies-EN.pdf)) from 342 individuals. We will be interested wheter it is possible to predict  cognitive religious belief style from personality traits. Make sure you downloaded the dataset from github repository and uploaded it into Colabolatory *Files*."
      ],
      "metadata": {
        "id": "kP1JP1zeZ9Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "8Q6dBZxA7WVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEyKd5686Pt8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import median_absolute_error, r2_score\n",
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "_7lj9aQbSmsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data_neo-ffi_religion.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GFeWUslzPjae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the dataset"
      ],
      "metadata": {
        "id": "Ob37VxKNVbL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "K--GIC2PdBFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "Let's see which personality traits are most associated with orthodox cognitive style. Create the model:\n",
        "\n",
        "*Orthodoxy ~ Extraversion + Agreeableness + Openness + Neuroticism + Conscientiousness*\n",
        "\n",
        "Fit the model using the training part of the data, then calculate predictions on the testing dataset. Calculate $R^2$ and MedianAE scores - you can use `compute_score()` method defined below. Then plot `y_true ~ y_predicted` to see how good your predictions are.\n",
        "\n",
        "There is a nice scikit-learn function for plotting true vs predicted values: [PredictionErrorDisplay.from_predictions()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html#sklearn.metrics.PredictionErrorDisplay.from_predictions)"
      ],
      "metadata": {
        "id": "MAfTPIBodKSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(y_true, y_pred):\n",
        "  '''\n",
        "  Helper function for printing scores.\n",
        "\n",
        "  Parameters:\n",
        "  y_true: ndarray of y values from original dataset.\n",
        "  y_pred: ndarray of y values predicted with given model.\n",
        "\n",
        "  Return:\n",
        "  dictionary object that consists of R2 and median absolute error scores.\n",
        "\n",
        "  '''\n",
        "  return {\n",
        "        \"R2\": f\"{r2_score(y_true, y_pred):.3f}\",\n",
        "        \"MedianAE\": f\"{median_absolute_error(y_true, y_pred):.3f}\",\n",
        "}"
      ],
      "metadata": {
        "id": "wRCBaKXFWC5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = # your code\n",
        "\n",
        "y = # your code\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# your code\n",
        "\n",
        "scores = compute_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "obUZ5NISPT02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = PredictionErrorDisplay.from_predictions(\n",
        "    # your code\n",
        "    kind='actual_vs_predicted',\n",
        ")"
      ],
      "metadata": {
        "id": "NrJWv15cSu90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our model is clearly not good. Do you have any idea what could be the reason? Take a close look at the `true vs predicted` plot and recall linear regression assumptioms. Why are they violated?\n",
        "\n",
        "HINT: Plot and then analyse the distribution of y data To plot distribution, you can use [`histplot()`](https://seaborn.pydata.org/generated/seaborn.histplot.html) from `seaborn`."
      ],
      "metadata": {
        "id": "3GZZwOtWZEv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "doZb6kKrZDYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Side note: There is another very useful method from `seaborn` that shows pairwise relationships in a dataset along with distributions.*"
      ],
      "metadata": {
        "id": "WO0neq1dagmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = sns.pairplot(df, kind=\"reg\", diag_kind=\"kde\")"
      ],
      "metadata": {
        "id": "m9l882jYX01f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From your graph, it is clear that the y variable (*Orthodoxy*) does not have a normal distribution, when features (independent variables) have. For linear models, **normal distribution of residuals (observed - predicted) is crucial. And this is often violated when your variables have different distributions.**"
      ],
      "metadata": {
        "id": "S-9Oqqu0bTIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Now you know that for a linear regression to be successful, you **might want to transform the non-normal data to have normal distributions**. Try again to model\n",
        "\n",
        "*Orthodoxy ~ Extraversion + Agreeableness + Openness + Neuroticism + Conscientiousness*\n",
        "\n",
        "but now, before fitting the model, transform your y data to have more Gaussian-like distrubutions. Compare `true vs predicted` plots, $R^2$ and $MAE$ of models before and after the transformation.\n",
        "\n",
        "HINT: There are automatic methods to make the data more Gauusian-like. Try googling (e.g. [stackoverflow](https://stackoverflow.com/questions/53624804/how-to-normalize-a-non-normal-distribution)) or use ChatGPT for help."
      ],
      "metadata": {
        "id": "MiJoAAxeb1E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see how skewness and kurtosis are big for Orthodoxy!\n",
        "summary = df.agg(['skew', 'kurtosis', 'mean', 'std', 'min', 'max']).transpose()\n",
        "summary"
      ],
      "metadata": {
        "id": "NF3L8mSxann2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed = df.copy()\n",
        "transformed_y = # your code\n",
        "\n",
        "df_transformed['Orthodoxy'] = transformed_y"
      ],
      "metadata": {
        "id": "_C_NRzyHs8P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see skewness and kurtosis after data transformation\n",
        "summary = df_transformed.agg(['skew', 'kurtosis', 'mean', 'std', 'min', 'max']).transpose()\n",
        "summary"
      ],
      "metadata": {
        "id": "r8XHdh9Ks-Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_transformed[[\n",
        "    'Extraversion',\n",
        "    'Agreeableness',\n",
        "    'Conscientiousness',\n",
        "    'Openness',\n",
        "    'Neuroticism']]\n",
        "\n",
        "y = df_transformed[['Orthodoxy']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# your code\n",
        "\n",
        "scores = compute_score(y_test, y_pred)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "c3Lo-neBbJhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "display_ = PredictionErrorDisplay.from_predictions(\n",
        "    y_test.to_numpy(),\n",
        "    y_pred,\n",
        "    kind=\"actual_vs_predicted\",\n",
        "    ax=ax,\n",
        "    scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "\n",
        "ax.set_title(\"Linear model\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "jc2jWN7vbJhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should see that the model estimated on normal data performs much better than the model estimated on exponential data."
      ],
      "metadata": {
        "id": "WjtQsOrmfztp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Extract coefficients from exercise 2 and plot them to see them better. Which trait has the greatest impact on orthodoxy?"
      ],
      "metadata": {
        "id": "kt2UhRC9fyRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "N9147M0wigcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting coefficients: scale matters\n",
        "\n",
        "Recall the scales of our features:"
      ],
      "metadata": {
        "id": "jla5t2hRjRPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.describe().transpose()"
      ],
      "metadata": {
        "id": "Cqw528MNjijI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do means and standard deviations of all features look similar? If not, you CANNOT compare coefficients of the model. An increase of 0.1 in variable a, 10 times larger than variable b, is not equal to an increase of 0.1 in variable b:\n",
        "\n",
        "```\n",
        "b = 1\n",
        "a = 10 * b\n",
        "\n",
        "a_01 = 0.1 * 10 = 1\n",
        "b_01 = 0.1 * 1 = 0.1\n",
        "```\n",
        "\n",
        "Thus, it is crucial for most models that features have similar scales (i.e. means and standard deviations). We cannot compare the magnitude of different coefficients since the features have different natural scales, and hence value ranges, e.g. because of their different unit of measure.\n",
        "\n",
        "*NEO-FFI Openness* clearly has different scale, thus coefficient next to this feature is not comparable to other coefficients. Look at standard deviation plot below.\n",
        "\n"
      ],
      "metadata": {
        "id": "0fGEADQEjxrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.std(axis=0).plot.barh(figsize=(9, 7))\n",
        "plt.title(\"Feature ranges\")\n",
        "plt.xlabel(\"Std. dev. of feature values\")"
      ],
      "metadata": {
        "id": "jL2VauHJjvsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing the feature set before modelling is an important step of data processing."
      ],
      "metadata": {
        "id": "YyiiZkOmnB2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4\n",
        "\n",
        "Create the same model as in exercises 1 and 2:\n",
        "\n",
        "*Orthodoxy ~ Extraversion + Agreeableness + Openness + Neuroticism + Conscientiousness*\n",
        "\n",
        "This time, before fitting, scale the feature set so that each feature has a similar scale; then compare coefficients of this model to coefficients from exercise 3. Use your transformed dataframe to correctly model a linear relationship.\n",
        "\n",
        "\n",
        "----\n",
        "Do you have an idea how to scale your data? Maybe you know some popular techniques?\n",
        "\n",
        "One of the most common ways to scale a vector of data is to subtract the mean of that vector from each element of the vector and divide the elements by the standard deviation of the vector. This results in the entire list having a mean of 0 and a standard deviation of 1. This kind of scaling can be done automatically with [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from scikit-learn. Use [`fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform) method to learn means and standard deviations of your features from the training dataset. Then use [`transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.transform) method to transform your testing data."
      ],
      "metadata": {
        "id": "rNj9EI_Zm98x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_transformed[[\n",
        "    'Extraversion',\n",
        "    'Agreeableness',\n",
        "    'Conscientiousness',\n",
        "    'Openness',\n",
        "    'Neuroticism']]\n",
        "\n",
        "y = df_transformed[['Orthodoxy']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vmdKqYQMojeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model with scaling"
      ],
      "metadata": {
        "id": "Izpwr7aJrTnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_transfromed = # your code\n",
        "\n",
        "# your code\n",
        "\n",
        "X_test_transformed = # your code\n",
        "\n",
        "scores = compute_score(y_test, y_pred)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "0Ekg6bnFcZuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot `true vs predicted`"
      ],
      "metadata": {
        "id": "9PhF1LfhrXTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "display_ = PredictionErrorDisplay.from_predictions(\n",
        "    y_test.to_numpy(), y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "\n",
        "ax.set_title(\"Linear model\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "dQhx3EEOcZud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract coefficients and then plot them"
      ],
      "metadata": {
        "id": "VFmNLz7BrbfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "HfSlpan4cZue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}