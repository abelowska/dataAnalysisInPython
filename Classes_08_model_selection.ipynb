{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abelowska/dataPy/blob/main/Classes_08_model_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UM_-fA16Wx0"
      },
      "source": [
        "# Model selection: in a search for the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook you will learn various techniques to help you in the model selection procedure:\n",
        "- [Cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/) procedure used to evaluate machine learning models on a limited data sample.\n",
        "- [Grid search](https://medium.com/fintechexplained/what-is-grid-search-c01fe886ef0a) used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n",
        "- Statistical tests for comparing models' performance."
      ],
      "metadata": {
        "id": "enOWPbOiDcUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Let's find the best model of *Orthodoxy ~ Extraversion+ Agreeableness + Conscientiousness + Openness + Neuroticism***"
      ],
      "metadata": {
        "id": "2EPl9szhQdRa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q6dBZxA7WVj"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEyKd5686Pt8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
        "\n",
        "import io\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import set_config\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for plotting\n",
        "plt.rcParams[\"figure.figsize\"] = (10,7)"
      ],
      "metadata": {
        "id": "CzCa4HEbJLru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Hsi48QOGpX"
      },
      "outputs": [],
      "source": [
        "# constants\n",
        "test_size=0.2\n",
        "random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(y_true, y_pred):\n",
        "  '''\n",
        "  Helper function for printing scores.\n",
        "\n",
        "  Parameters:\n",
        "  y_true: ndarray of y values from original dataset.\n",
        "  y_pred: ndarray of y values predicted with given model.\n",
        "\n",
        "  Return:\n",
        "  dictionary object that consists of R2 and median absolute error scores.\n",
        "\n",
        "  '''\n",
        "  return {\n",
        "        \"R2\": r2_score(y_true, y_pred),\n",
        "        \"MedianAE\": median_absolute_error(y_true, y_pred),\n",
        "}"
      ],
      "metadata": {
        "id": "FHbXo8iS9wlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXa9GglZ6NcC"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNjW37zq6NcE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data_neo-ffi_religion.csv')\n",
        "df['Orthodoxy'] = np.log(df[['Orthodoxy']].to_numpy())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsu1bvMHf2S8"
      },
      "source": [
        "### Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the dataset"
      ],
      "metadata": {
        "id": "2_s0HU7J6BRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_ado_rVXzyc"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create classes"
      ],
      "metadata": {
        "id": "x7sNgMUob88S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['class'] = df[['External Critique', 'Orthodoxy', 'Relativism', 'Second Naïveté']].idxmax(axis=1)"
      ],
      "metadata": {
        "id": "EwycU5gxb9E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "_YoM4dGwcNWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select X and y sets"
      ],
      "metadata": {
        "id": "B4pio9V91tuh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ_cFgtpgqsG"
      },
      "outputs": [],
      "source": [
        "X = df[[\n",
        "    'Extraversion',\n",
        "    'Agreeableness',\n",
        "    'Conscientiousness',\n",
        "    'Openness',\n",
        "    'Neuroticism'\n",
        "    ]]\n",
        "\n",
        "y = df[['Orthodoxy']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwTevc1PjfK4"
      },
      "source": [
        "Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-kDCEKhhEry"
      },
      "outputs": [],
      "source": [
        "# to ensure repeatability of splits, we set the random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state\n",
        ")\n",
        "\n",
        "print(f\"Shape of the X train dataset: {X_train.shape}\")\n",
        "print(f\"Shape of the X test dataset: {X_test.shape}\")\n",
        "print(f\"Shape of the y train dataset: {y_train.shape}\")\n",
        "print(f\"Shape of the y test dataset: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Best model: Problem 1 - Choosing\n",
        "\n",
        "It is quite obvious that we would like to find the best model of *Orthodoxy vs Big Five*. As we saw during the last classes for e.g., SVM we can set the values at least two hyperparameters - and there is a lot of values to test! What model - what hyperparameters - are the best?\n",
        "\n",
        "**And what does it mean - *\\\"the best model\\\"*?**\n",
        "\n",
        "As everywhere, we have to have a metric that says what is better. Until now, when we said *\\\"the best model\\\"*, we were comparing the results on a **test set**. But the test set should not be used for comparison.\n",
        "\n",
        "What is the test set for?\n",
        "\n",
        "We should use the test set only for the final evaluation of the best model. Otherwise, there is a leakage of knowledge from the test set - it is used to choose the best model (i.e. fit!) not for pure evaluation.\n",
        "\n",
        "So - we have to choose the best model to and **then** perform a final testing. Let's try to do this!"
      ],
      "metadata": {
        "id": "U3EwurY7BMNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1. Comparing models based on the training dataset\n",
        "\n",
        "Create a list of kernels to test and C to test (and epsilons to test) and using for loops try to find the best SVM model.Use only **training set** to choose the best SVM model."
      ],
      "metadata": {
        "id": "9sQvJYub8UMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "xSVZUaqr9IDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "8vOx4tF2-bcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaXGezVOqysO"
      },
      "source": [
        "Which model is the best?\n",
        "\n",
        "Let's check the validation (test) scores of models and check if we were right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz8lVVULqQd0"
      },
      "outputs": [],
      "source": [
        "results_test_df = results_df.copy()\n",
        "\n",
        "# create an empty column for r2 score on test set\n",
        "results_test_df['test_r2_score'] = 0\n",
        "\n",
        "for idx, row in results_df.iterrows():\n",
        "  # get model form dataframe\n",
        "  model = row['model']\n",
        "\n",
        "  # evaluate model on the test set\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  r2 = compute_score(y_test, y_test_pred)['R2']\n",
        "\n",
        "  # save results to dataframe\n",
        "  results_test_df.at[idx, 'test_r2_score'] = r2\n",
        "\n",
        "display(results_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP6TmgYsZ58f"
      },
      "source": [
        "It is clear that some models are overfitted.\n",
        "\n",
        "\n",
        "**Obviously, given the phenomenon of overfitting, the model with the best score on the training set is not the model with the best score on the test set!!!**\n",
        "\n",
        "Cross-validation provides better estimates of the real prodictive performance of tested models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sIrA9l_aIbK"
      },
      "source": [
        "### Exercise 2. Cross-validation\n",
        "See the documentation of calculating the cross-validated scores in scikit-learn: [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) and then:\n",
        "1. Add estimation of the cross-validated scores to the code below.\n",
        "2. Add the mean CV score to the dataframe with the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CoR_euiZjKa"
      },
      "outputs": [],
      "source": [
        "C_list = [0.0001, 0.1, 1.0, 10000]\n",
        "epsilon_list = [0.1, 1, 2]\n",
        "kernel_list = ['linear', 'rbf']\n",
        "\n",
        "results_cv_df = pd.DataFrame()\n",
        "\n",
        "for C in C_list:\n",
        "  for epsilon in epsilon_list:\n",
        "    for kernel in kernel_list:\n",
        "      estimator = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
        "\n",
        "      # create pipeline\n",
        "      model = make_pipeline(StandardScaler(), estimator)\n",
        "\n",
        "      # fit model\n",
        "      model.fit(X_train, y_train.to_numpy().ravel())\n",
        "\n",
        "      # predict on the training data\n",
        "      y_train_pred = model.predict(X_train)\n",
        "      r2 = compute_score(y_train, y_train_pred)['R2']\n",
        "\n",
        "      # compute cross validated r2 scores on train data\n",
        "      k_folds = 3\n",
        "      cv_scores = # todo\n",
        "      mean_cv_score = np.mean(cv_scores)\n",
        "\n",
        "      # define model name\n",
        "      model_name = f\"kernel: {kernel}, C:{C}, epsilon: {epsilon}\"\n",
        "\n",
        "      # save results in dataframe\n",
        "      this_result = pd.DataFrame({\n",
        "          \"model_name\": [model_name],\n",
        "          \"train_r2_score\": [r2],\n",
        "          'cv_r2_score': # todo\n",
        "          \"model\": [model],\n",
        "      })\n",
        "\n",
        "      results_cv_df = pd.concat([results_cv_df, this_result], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0poymX4ZbMcO"
      },
      "outputs": [],
      "source": [
        "results_cv_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QcgNXYKGYqm"
      },
      "source": [
        "Which model is the best?\n",
        "\n",
        "Let's check the validation (test) scores of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JMQ1ZD1GYqn"
      },
      "outputs": [],
      "source": [
        "results_cv_test_df = results_cv_df.copy()\n",
        "\n",
        "# create an empty column for r2 score on test set\n",
        "results_cv_test_df['test_r2_score'] = 0\n",
        "\n",
        "for idx, row in results_cv_df.iterrows():\n",
        "  # get model form dataframe\n",
        "  model = row['model']\n",
        "\n",
        "  # evaluate model on the test set\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  r2 = compute_score(y_test, y_test_pred)['R2']\n",
        "\n",
        "  # save results to dataframe\n",
        "  results_cv_test_df.at[idx, 'test_r2_score'] = r2\n",
        "\n",
        "display(results_cv_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at $R^2$ *~ models* plot. You can see that CV scores more or less follow test scores, **especially for the overfitting case**."
      ],
      "metadata": {
        "id": "PDHOh2jF8Txi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_melted = pd.melt(results_cv_test_df, id_vars=['model_name'], value_vars=['train_r2_score', 'cv_r2_score', 'test_r2_score'])\n",
        "\n",
        "sns.lineplot(df_melted, y='value', x='model_name', hue='variable')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('R2 Scores Comparison')\n",
        "plt.xlabel('Model Name')\n",
        "plt.ylabel('R2 Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "MsBIf4CvGEBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ZHcarNcQSo"
      },
      "source": [
        "Now we see that with cross-validation we better choose our best model. CV score is much more reliable than a simple test score. This is very important especially when comparing different models before final testing.\n",
        "- **Relying only on the train score can lead to an overestimation of the model's performance and predictive power.**\n",
        "- **Relying on the test score lead to knowledge leaking and loss of generalisability.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Exercise 2.1) Compare mean CV scores for different number of folds\n",
        "\n",
        "Models fit always depends on the dataset (as you remember from one of the homeworks). The more folds, the larger the set the model fits on, but the smaller the set for internal testing. Test whether (and how) the number of folds, and therefore the size of the internal sets for training and testing, affects the quality of the model. Compare the results to the r2 of the training and testing set."
      ],
      "metadata": {
        "id": "quJXUER4CNRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "PBKDa8DFC_C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your plotting code"
      ],
      "metadata": {
        "id": "7eOYeCBFD7Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Best model: Problem 2 - Searching and choosing\n",
        "\n",
        "Using the SVR example, we saw that in certain estimators we can set certain parameters that change the way the data is fit. These parameters are called **hyperparameters** of the model.\n",
        "\n",
        "It is intuitive, that there is an optimal composition of hyperparameters that yield the best score. It is quite hard to find optimal hyperparameters for an estimator, considering how huge a space we have to search.\n",
        "\n",
        "Finding optimal hyperparameters is an optimization problem - and there are some techniques that help us to search over specified hyperparameters values for an estimator.\n",
        "\n",
        "One of such techniques is so called [`Grid Search`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). `Grid Search` performs exhaustive search over specified parameter values for an estimator.\n",
        "\n",
        "Let's try to use `GridSearch` to find optimal hyperparameters values for estimators.\n",
        "\n",
        "- `GridSearch` works exactly the same as `Pipelines` - we can `fit()` and `predict()` on `GridSearch` object.\n",
        "- `GridSearch` is parametrised with so called `param_grid` - the dictionary of parameters, where keys are estimator's parameters (hyperparameters) and values are lists of huperparameters' values to test, e.g.:\n",
        "\n",
        "```\n",
        "svr_params = dict(\n",
        "    svr__kernel=[\"rbf\"],\n",
        "    svr__C=[1, 10],\n",
        "    svr__epsilon=[2, 5, 10],\n",
        ")\n",
        "```\n",
        "- You can pass `scoring` parameter to `GridSearch`, to set the metric of performance evaluation, e.g., `scoring = \"r2\"`.\n",
        "- You can pass the strategy of CV splitting, or just the number of folds, e.g., `cv=5`.\n",
        "\n"
      ],
      "metadata": {
        "id": "cT3HusBRHbYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3: Serch for SVR best hyperparameters"
      ],
      "metadata": {
        "id": "8xlZxrGuP__U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svr_params = # define a grid of parameters to be searched"
      ],
      "metadata": {
        "id": "po1VK7349dIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create pipeline\n",
        "model = make_pipeline(StandardScaler(), SVR())\n",
        "\n",
        "# define cross-validation k\n",
        "cv_kf = # todo\n",
        "\n",
        "# define grid search\n",
        "grid_search_model = GridSearchCV(\n",
        "    # todo\n",
        ")\n",
        "\n",
        "\n",
        "# fit model\n",
        "grid_search_model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_test_pred = grid_search_model.predict(X_test)\n",
        "test_score = r2_score(y_test, y_test_pred)\n",
        "print(f'Test R2 score: {test_score}')\n",
        "\n",
        "# predict on train data\n",
        "y_train_pred = grid_search_model.predict(X_train)\n",
        "train_score = r2_score(y_train, y_train_pred)\n",
        "print(f'Train R2 score: {train_score}')\n",
        "\n",
        "# extract mean cv scores\n",
        "mean_cv_score = grid_search_model.best_score_\n",
        "print(f'CV mean R2 score: {mean_cv_score}')"
      ],
      "metadata": {
        "id": "06W61WU4FxKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "View results of Grid Search:"
      ],
      "metadata": {
        "id": "Tk0IxauCmhCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Choosen model: {grid_search_model.best_estimator_}\\n\")\n",
        "print(f\"Choosen hyperparameters: {grid_search_model.best_params_}\\n\")\n",
        "print(f\"Train score: {train_score} \\nTest score: {test_score}\")"
      ],
      "metadata": {
        "id": "kmZ8SK6TliaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the Grid Search results in details:"
      ],
      "metadata": {
        "id": "8nLtByBkkwlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results_df = pd.DataFrame(grid_search_model.cv_results_)\n",
        "cv_results_df"
      ],
      "metadata": {
        "id": "0skDFb838GxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### (Exercise 3.1) Potting GS results\n",
        "Try to plot results of the grid search to see how the values of the given hyperparameters affected the performance of the model. Plot e.g., $R^2$ *~ epsilon*, with hue on C values."
      ],
      "metadata": {
        "id": "SSGcN6-jEXHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "EGJW2GhnGFIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "1ZLQaptpItbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results_df.iloc[[grid_search_model.best_index_]]"
      ],
      "metadata": {
        "id": "7O-Nhj9S8vqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_splits_scores = cv_results_df.filter(\n",
        "    regex=r\"split\\d*_test_r2\").iloc[grid_search_model.best_index_]"
      ],
      "metadata": {
        "id": "vu2yo-LB9RX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best estimator: {grid_search_model.best_estimator_}\\n\")\n",
        "print(f\"Best parameters: {grid_search_model.best_params_}\\n\")\n",
        "print(f\"Best mean CV score: {grid_search_model.best_score_}\\n\")\n",
        "print(f\"CV scores:\\n{cv_splits_scores}\")"
      ],
      "metadata": {
        "id": "nYnLcYWCjzMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we know the values of hyperparameters of the model with the best mean CV score. And - most importantly - we have chosen our model without knowledge leakage for the testing set.**"
      ],
      "metadata": {
        "id": "C84RNI1JouDg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}