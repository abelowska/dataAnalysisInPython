{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abelowska/dataPy/blob/main/Classes_06_KNN_DT_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UM_-fA16Wx0"
      },
      "source": [
        "# Classification: KNN and Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q6dBZxA7WVj"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEyKd5686Pt8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, balanced_accuracy_score\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
        "\n",
        "import io\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import set_config\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import power_transform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,7)"
      ],
      "metadata": {
        "id": "CzCa4HEbJLru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Hsi48QOGpX"
      },
      "outputs": [],
      "source": [
        "# constans\n",
        "test_size=0.2\n",
        "random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score_classification(y_true, y_pred):\n",
        "  '''\n",
        "  Helper function for printing scores.\n",
        "\n",
        "  Parameters:\n",
        "  y_true: ndarray of y values from original dataset.\n",
        "  y_pred: ndarray of y values predicted with given model.\n",
        "\n",
        "  Return:\n",
        "  dictionary object that consists of accuracy and classification report.\n",
        "\n",
        "  '''\n",
        "  return {\n",
        "        \"Accuracy\": f\"{accuracy_score(y_true, y_pred):.3f}\",\n",
        "        \"Classification Report\": classification_report(y_true, y_pred),\n",
        "}"
      ],
      "metadata": {
        "id": "weAtuf398bDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C40f350QVvNb"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEsePeoHLXCj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data_neo-ffi_religion.csv')\n",
        "df['Orthodoxy'] = np.log(df[['Orthodoxy']].to_numpy())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the dataset"
      ],
      "metadata": {
        "id": "2_s0HU7J6BRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_ado_rVXzyc"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1"
      ],
      "metadata": {
        "id": "Hy_V0n_1-Jfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the model from the last classes:\n",
        "\n",
        "*Orthodoxy ~ Extraversion + Agreeableness + Openness + Neuroticism + Conscientiousness*\n",
        "\n",
        "Now we will perform a classification of our data: based on the results of the Big Five, we will predict membership in one of four cognitive approaches to belief (*Orthodoxy, External Critique, Second Naïveté, and Relativism*).\n",
        "\n",
        "To perform classification, we have to create classes. Each participant (sample) must be of a known class.\n",
        "\n",
        "Create a new column called `'Class'` that store the correct class for a given observation. **Assume that the class of a given observation corresponds to the cognitive style that has the highest value**."
      ],
      "metadata": {
        "id": "1KNDWKxtA0QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "mEfNhN2HzB8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Nearest Neighbours"
      ],
      "metadata": {
        "id": "2x6HkzGs6Y-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have data in the correct format, so we can begin to create a model. Let's statrt from KNN Classifier model. Look into the documentation of [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and write down the code, employing the same patter as in the regression analysis. Do not forget to scale your data (e.g., using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n",
        "To check the classification results, use the predefined `compute_score_classification()` method and print separately each metric. How you interpret the results of the model?"
      ],
      "metadata": {
        "id": "gyFUy1YvCFZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "KoaxKB0gC0Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees"
      ],
      "metadata": {
        "id": "dcf6CPqORd4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use Decision Tree classifier to predict the PCBS classes. See the documantation of [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and create DT model. Do not forget to scale your data (e.g., using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). Compare results of DT and KNN. Which classifier seems to work better?\n",
        "\n",
        "\n",
        "Save the structure of your DT  into `.dot` file and visualize it using the [WebGraphviz](http://www.webgraphviz.com) tool. You should copy the content of the `.dot` file (saved to the *Files* directory in Colab) to the input area on the [WebGraphviz](http://www.webgraphviz.com)."
      ],
      "metadata": {
        "id": "KS0l8nqMD7VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "KhFI5zciAvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Exercise 2)\n",
        "\n",
        "Now, recall the theory behind the Post-Critical Belief Scale [source](https://theo.kuleuven.be/apps/press/ecsi/files/2019/03/4.-Pollefeyt-Bouwens-PCB-Melb-Vict-for-dummies-EN.pdf). Four classes of cognitive approaches to belief are defined by two dimensions: Exclusion vs. Inclusion of Transcendence and Iteral vs. Symbolic Belief. Defining the class based on the highest value can be suboptimal (it somehow assumes perfect introspection). Think how such two dimensions could be created from the data you have, assuming the theory is (reasonably) correct. Try to define these dimensions and check, whether classification results are improved.\n",
        "\n",
        "HINT: Think of the values of the four PCBS classes as vectors. Which values should be summed up and which subtracted to obtain Literal/Symbolic and Inclusion/Exclusion dimensions?"
      ],
      "metadata": {
        "id": "0ucLf4hE-MlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "39oh9yXaAbAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "When you created the KNN and DT model - most of the code (actually all of it, except for the line defining the model) was the same. This is quite a waste of time and space. It also makes it difficult to read, analyze, and refactor the code. The [`Pipelines`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) framework was created exactly for such situations. As sugested by the name, `Pipeline` is a pipe of transforms (functions that somehow transform the data) with a final estimator at the end. According to the documentation, intermediate steps of the pipeline must be *transforms*, that is, they must implement `fit` and `transform` methods (e.g., `StandardScaler`). The final estimator only needs to implement `fit` (e.g., `KNeighborsClassifier`). When you create a pipeline, you can think of this pipeline as a model - in fact, individual data processing steps are already a model, such as scaler, because they often learn from data.\n",
        "\n",
        "For the sake of simplicity, we'll start with the [`make_pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline) function, which conveniently allows you to create a pipeline. Take a look at the example below:\n",
        "\n",
        "```\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))\n",
        "model.fit(X,y)\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "Again, create KNN and DT classifiers, but this time:\n",
        "1. Define classification estimators beforehand and put them in a list;\n",
        "2. Use a for loop to ...\n",
        "3. ... make pipeline that chain scaler with estimator using `make_pipeline()` function."
      ],
      "metadata": {
        "id": "fCHgmIDMFu8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "aEao3IUT4uJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework:"
      ],
      "metadata": {
        "id": "YskIR6RAadkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we will examine how the number of features analyzed and the value of the model's hyperparameter(s) are related to accuracy. Such an analysis will allow us, for example, to answer the question of how many personality traits (from NEO-FFI) we need to have in order to create a valid model that predicts cognitive belief style. Perhaps we can achieve a similar results with much less data? Alongside, we will examine the effect of hyperparameter values on accuracy depending on the number of features. To do so, you want to randomly select a subset of features (think - why randomly?)from a set of features and test the model's performance on that feature set for, e.g., set of n_neighbors.\n",
        "\n",
        "**HINT**: You may want to follow the step list below (for KNN):\n",
        "\n",
        "1. Define a list of all possible features (all five scales form NEO-FFI);\n",
        "2. Define a list specifying the number of features you might select (intuitively, it's a list from 1 to 5);\n",
        "3. Define a list specifying the n_neighbors to be tested (e.g., from 1 to 20);\n",
        "4. For each number/size from the list with possible number of features:\n",
        "  \n",
        "  4.1. Draw random subset of features (features names) of this size. To do so, you can use `random.sample()` in a following way: `random.sample(list_of_all_possible_features, number_of_features_to_select)` ;\n",
        "  \n",
        "  4.2. Create the y set, and X set based on the output of `random.sample()`;\n",
        "  \n",
        "  4.3. Perform train-test split, scale X_train and X_test, perform classification, estimate the accuracy (e.g., `accuracy_score(y_true, y_pred)`), and the average precision (e.g., `precision_score(y_test, y_pred, average='weighted')`);\n",
        "\n",
        "  4.4. Save to results of classification to a dataframe: the number of features that was used in classification, value of k, accuracy, and the average precision;\n",
        "\n",
        "5. Do all steps from the (4) n=50 (or more) times and for each k from your list of n_neighbors;\n",
        "6. Plot the resultson accuracy and average precision using e.g. `sns.lineplot()` with hue set to number of features.\n",
        "\n",
        "Do not forget put your comments about the effect of the number of features on accuracy and the relationship between the accuracy and the number of k depending on number of features. Did you learn anything interesting about the relationship between personality traits and cognitive belief style?"
      ],
      "metadata": {
        "id": "A3WkUZb-fLkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "BJz5gvAt2wEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}